{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy\n",
    "from os.path import join\n",
    "from umap import UMAP\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"browser\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../data/tweets\"\n",
    "fname = \"combined_US_politician_twitter_timelines_2010-11-06_to_2022-03-16_lemmatized_text.csv.gzip\"\n",
    "data = pd.read_csv(join(src, fname), encoding=\"utf-8\").dropna()\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the data and creating a list of docs for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text = data.apply(lambda row: re.sub(r\"http\\S+\", \"\", str(row.text)).lower(), 1)\n",
    "docs = data.text.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model (increase `n_neigbors` and/or `min_topic_size` if too many topics):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 48206/48206 [3:12:49<00:00,  4.17it/s]  \n",
      "2022-05-05 12:37:20,186 - BERTopic - Transformed documents to Embeddings\n",
      "2022-05-05 21:51:38,008 - BERTopic - Reduced dimensionality with UMAP\n",
      "2022-05-05 22:02:19,430 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2022-05-05 22:06:19,405 - BERTopic - Reduced number of topics from 485 to 378\n"
     ]
    }
   ],
   "source": [
    "umap_model = UMAP(n_neighbors=100, n_components=5, metric='cosine', low_memory=False)\n",
    "vectorizer_model = CountVectorizer(min_df=50)\n",
    "topic_model = BERTopic(verbose=True, nr_topics=\"auto\", min_topic_size=200, umap_model=umap_model, top_n_words=20, vectorizer_model=vectorizer_model)\n",
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving and loading the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.save(\"twitter_lemmatized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic.load(\"twitter_lemmatized\") #insert path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking how many topics the model found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>867530</td>\n",
       "      <td>-1_to_and_in_be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>22275</td>\n",
       "      <td>0_obamacare_aca_care_condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>18886</td>\n",
       "      <td>1_voting_vote_ballot_election</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>14776</td>\n",
       "      <td>2_abortion_woman_right_life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>12707</td>\n",
       "      <td>3_gun_violence_background_shooting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>372</td>\n",
       "      <td>206</td>\n",
       "      <td>372_heart_disease_february_awareness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>373</td>\n",
       "      <td>206</td>\n",
       "      <td>373_hand_produce_donate_covid19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>374</td>\n",
       "      <td>205</td>\n",
       "      <td>374_rush_radio_conservative_icon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>375</td>\n",
       "      <td>204</td>\n",
       "      <td>375_conflict_condemn_civilian_violation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>376</td>\n",
       "      <td>202</td>\n",
       "      <td>376_texas_independence_otd_republic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>378 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic   Count                                     Name\n",
       "0       -1  867530                          -1_to_and_in_be\n",
       "1        0   22275           0_obamacare_aca_care_condition\n",
       "2        1   18886            1_voting_vote_ballot_election\n",
       "3        2   14776              2_abortion_woman_right_life\n",
       "4        3   12707       3_gun_violence_background_shooting\n",
       "..     ...     ...                                      ...\n",
       "373    372     206     372_heart_disease_february_awareness\n",
       "374    373     206          373_hand_produce_donate_covid19\n",
       "375    374     205         374_rush_radio_conservative_icon\n",
       "376    375     204  375_conflict_condemn_civilian_violation\n",
       "377    376     202      376_texas_independence_otd_republic\n",
       "\n",
       "[378 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the topics (call `x.write_html(path)` to save the graphs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_heatmap(n_clusters=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Topic Modelling (topics over time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = topic_model.transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = data.created_at.to_list()\n",
    "topics_over_time = topic_model.topics_over_time(docs, topics, timestamps, nr_bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating topics per class dataframe to check for topic distribution over party/components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_per_class = topic_model.topics_per_class(docs, topics=topics, classes=data[\"classes\"], global_tuning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics_per_class(topics_per_class, top_n_topics=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = \"../../data/tweets\"\n",
    "fname = \"topics_per_class.csv\"\n",
    "topics_per_class.to_csv(join(dst, fname), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data for wordclouds (use with specific topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_ids = [] #insert topic id(s)\n",
    "topics_words = []\n",
    "topic_embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in topics_ids:\n",
    "    word_list = []\n",
    "    embedding_list = []\n",
    "    for i in range(0,20): # insert how many words you want to display (=< top_n_words in model computation)\n",
    "        word = topic_model.get_topic(id)[i][0]\n",
    "        word_list.append(word)\n",
    "        embedding = topic_model.get_topic(id)[i][1]\n",
    "        embedding_list.append(embedding)\n",
    "    topics_words.append(word_list)\n",
    "    topic_embeddings.append(embedding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_df = pd.DataFrame()\n",
    "wordcloud_df[\"topic_ids\"] = topics_ids\n",
    "wordcloud_df[\"topic_words\"] = topics_words\n",
    "wordcloud_df[\"topic_embeddings\"] = topic_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = \"../../data/tweets\"\n",
    "fname = \"key_topics.csv\"\n",
    "wordcloud_df\\\n",
    "    .set_index([\"topic_ids\"])\\\n",
    "    .apply(pd.Series.explode)\\\n",
    "    .reset_index()\\\n",
    "    .to_csv(join(dst, fname), index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3cf53b4603f886e33247712558f5be854cca975a28ccddf843ec0f2f06601796"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
