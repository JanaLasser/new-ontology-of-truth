{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "from umap import UMAP\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"browser\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and wrangle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "src = \"../../data/tweets\"\n",
    "fname = \"combined_US_politician_twitter_timelines_2010-11-06_to_2022-03-16_lemma.csv.gzip\"\n",
    "tweets = pd.read_csv(\n",
    "    Path(src, fname),\n",
    "    compression=\"gzip\",\n",
    "    dtype={\"id\":str}\n",
    ")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data and creating a list of docs for the model\n",
    "tweets['lemmatized'] = tweets['lemmatized'].replace(r'\\s+|\\\\n', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = tweets.lemmatized.to_list()\n",
    "docs = [item for item in docs if str(item) != 'nan']\n",
    "filtered_data = data.dropna(subset=[\"lemmatized\"])\n",
    "len(filtered_data) == len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase `n_neigbors` and/or `min_topic_size` if too many topics:\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=150, \n",
    "    n_components=5, \n",
    "    metric=\"cosine\", \n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "vectorizer_model = CountVectorizer(min_df=50)\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    verbose=True,\n",
    "    nr_topics=\"auto\",\n",
    "    min_topic_size=200,\n",
    "    umap_model=umap_model,\n",
    "    top_n_words=10,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    language=\"english\"\n",
    ")\n",
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "dst = \"../../data/tweets\"\n",
    "fname = \"BERTopic_model\"\n",
    "topic_model.save(Path(dst, fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export topic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a table with docs and respective topics\n",
    "df_all_docs = pd.DataFrame({\n",
    "    \"id\": filtered_data[\"id\"], \n",
    "    'topic': topics\n",
    "})\n",
    "\n",
    "# add belief-speaking and truth-seeking similarity\n",
    "src = \"../../data/tweets\"\n",
    "fname = \"US_politician_tweets_2010-11-06_to_2022-03-16.csv.gzip\"\n",
    "cols = [\"id\", \"avg_belief_score\", \"avg_truth_score\"]\n",
    "similarities = pd.read_csv(\n",
    "    Path(src, fname),\n",
    "    usecols=cols,\n",
    "    compression=\"gzip\",\n",
    "    dtype={\"id\":str}\n",
    ")\n",
    "df_all_docs = pd.merge(\n",
    "    df_all_docs,\n",
    "    similarities,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# save for plotting belief-speaking similarity and truth-seeking\n",
    "# similarity in figure 1\n",
    "dst = \"../../data/tweets\"\n",
    "fname = \"topics_all_docs.csv.gzip\"\n",
    "df_all_docs.to_csv(\n",
    "    Path(dst, fname),\n",
    "    compression=\"gzip\",\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../data/tweets\"\n",
    "fname = \"BERTopic_model\"\n",
    "topic_model = BERTopic.load(Path(src, fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many topics the model found\n",
    "dst = \"../../data/tweets\"\n",
    "fname = \"topics_info.csv\"\n",
    "topic_info = pd.DataFrame(topic_model.get_topic_info())\n",
    "topic_info.to_csv(Path(dst, fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the topics (call `x.write_html(path)` to save the graphs)\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_heatmap(n_clusters=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Topic Modelling (topics over time):\n",
    "topics, probs = topic_model.transform(docs)\n",
    "timestamps = data.created_at.to_list()\n",
    "topics_over_time = topic_model.topics_over_time(docs, topics, timestamps, nr_bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating topics per class dataframe to check for topic distribution over \n",
    "# party/components\n",
    "topics_per_class = topic_model.topics_per_class(\n",
    "    docs=docs, \n",
    "    topics=topics, \n",
    "    classes=filtered_data[\"classes_quant\"], \n",
    "    global_tuning=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics_per_class(topics_per_class, top_n_topics=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save topic names for plotting in figure 1\n",
    "dst = \"../../data\"\n",
    "fname = \"topics_per_class_ddr.csv\"\n",
    "topics_per_class_named.to_csv(Path(dst, fname), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data to create the scattertext plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_ids = []  # insert topic id(s)\n",
    "topics_words = []\n",
    "topic_embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in topics_ids:\n",
    "    word_list = []\n",
    "    embedding_list = []\n",
    "    for i in range(\n",
    "        0, 20\n",
    "    ):  # insert how many words you want to display (=< top_n_words in model computation)\n",
    "        word = topic_model.get_topic(id)[i][0]\n",
    "        word_list.append(word)\n",
    "        embedding = topic_model.get_topic(id)[i][1]\n",
    "        embedding_list.append(embedding)\n",
    "    topics_words.append(word_list)\n",
    "    topic_embeddings.append(embedding_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_df = pd.DataFrame()\n",
    "wordcloud_df[\"topic_ids\"] = topics_ids\n",
    "wordcloud_df[\"topic_words\"] = topics_words\n",
    "wordcloud_df[\"topic_embeddings\"] = topic_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = \"../../tweets\"\n",
    "fname = \"key_topics.csv\"\n",
    "wordcloud_df = wordcloud_df.set_index([\"topic_ids\"]).apply(pd.Series.explode).reset_index()\n",
    "wordcloud_df.to_csv(Path(dst, fname))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "260abd549318bbbd6a07931d7c19284ef86ae3047ff26637378d5cbd84eee02c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
