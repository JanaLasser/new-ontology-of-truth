{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1316acf0-df86-4849-bfce-ae80cceac5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# author: Jana Lasser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26d70086-b0cd-471d-867d-21e782483b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139da757-f9f3-4839-b4e8-224adf5dafd7",
   "metadata": {},
   "source": [
    "# Create a URL data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bc1b3-3813-489c-8aa9-266ddf21b8fe",
   "metadata": {},
   "source": [
    "## Expand URL lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d47cd12-d37e-48b2-9c2a-762a6cedc62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cleaned timeline-data\n",
    "src = \"../../data/tweets\"\n",
    "fname = \"combined_US_politician_twitter_timelines_2010-11-06_to_2022-03-16_clean.csv.gzip\"\n",
    "cols = [\"id\", \"author_id\", \"created_at\", \"expanded_urls\",\n",
    "        \"retweeted\", \"quoted\", \"reply\"]\n",
    "tweets = pd.read_csv(\n",
    "    join(src, fname),\n",
    "    compression=\"gzip\",\n",
    "    usecols=cols)\n",
    "tweets = tweets.drop_duplicates(subset=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ffcd134-eaad-4cc9-b662-6f0067f2d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the URL lists\n",
    "tweets[\"expanded_urls\"] = tweets[\"expanded_urls\"].fillna(\"[]\")\n",
    "tweets[\"expanded_urls\"] = tweets[\"expanded_urls\"].apply(lambda x: eval(x))\n",
    "tweets[\"has_url\"] = tweets[\"expanded_urls\"].apply(lambda x: len(x) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "696f9f53-bd91-4e8c-adb7-592e7199f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"N_urls\"] = tweets[\"expanded_urls\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abb66c73-f676-4875-ad0c-2a46a99cc4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand only entries with multiple URLs\n",
    "multiple_urls = tweets[tweets[\"N_urls\"] > 1]\n",
    "expanded_urls = pd.DataFrame()\n",
    "for idx, entry in multiple_urls.iterrows():\n",
    "    row = {key:val for key, val in entry.items()}\n",
    "    expanded_urls = pd.concat([expanded_urls, pd.DataFrame(row)])\n",
    "    \n",
    "expanded_urls = expanded_urls.set_index(\"id\")\n",
    "urls = tweets.copy()\n",
    "urls = urls.set_index(\"id\")\n",
    "# drop entries with mutiple URLs\n",
    "urls = urls.drop(multiple_urls[\"id\"].values)\n",
    "# add expanded entries with one line for each URL\n",
    "urls = pd.concat([urls, expanded_urls])\n",
    "urls = urls.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a03594ce-f9b9-43f3-9b95-fa5ecf9dfac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2377899"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910bb11-3cf4-4fec-a136-795b73229929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, some URLs are stored as singular entries of a list, and some as string.\n",
    "# empty entries are stored as empty list. Below we streamline URL entries such\n",
    "# that every entry is a single string\n",
    "def extract_URL_from_list(entry):\n",
    "    if len(entry) == 0:\n",
    "        return np.nan\n",
    "    elif len(entry) == 1:\n",
    "        return entry[0]\n",
    "    else:\n",
    "        return entry\n",
    "    \n",
    "urls[\"expanded_urls\"] = urls[\"expanded_urls\"].apply(extract_URL_from_list)\n",
    "urls = urls.drop(columns=[\"urls\", \"entities.urls\"])\n",
    "urls = urls.rename(columns={\"expanded_urls\":\"url\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3a37e86-396c-4706-ac02-0b3f4f2c5ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped 182435 duplicate URL entries\n"
     ]
    }
   ],
   "source": [
    "# some tweets contain the same URL twice. We drop these\n",
    "N = len(urls)\n",
    "urls = urls.drop_duplicates(subset=[\"id\", \"url\"])\n",
    "print(f\"dropped {N - len(urls)} duplicate URL entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e5cb331-3aee-4fe9-b6ea-d8ec5b0d35ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ba76e8-09a0-4f2e-bd7c-198a30a14e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the outcome\n",
    "dst = \"../../data/urls\"\n",
    "fname = \"combined_US_politician_twitter_timelines_2010-11-06_to_2021-03-16_clean_urls.csv.gzip\"\n",
    "urls.to_csv(join(dst, fname), compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9287984-bfed-41cd-b853-226df27367c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data frame with the expanded URLs\n",
    "src = \"../../data/urls\"\n",
    "fname = \"combined_US_politician_twitter_timelines_2010-11-06_to_2021-03-16_clean_urls.csv.gzip\"\n",
    "cols = [\"id\", \"author_id\", \"created_at\", \"url\", \"retweeted\",\n",
    "        \"quoted\", \"reply\", \"has_url\"]\n",
    "urls = pd.read_csv(\n",
    "    join(src, fname),\n",
    "    compression=\"gzip\",\n",
    "    usecols=cols,\n",
    "    parse_dates=[\"created_at\"],\n",
    "    dtype={\"author_id\":str, \"id\":str}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07371d18-7280-4dc1-9723-fc500507ec3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsrc = \"../../data/tweets\"\\nfname = \"combined_US_politician_twitter_timelines_2010-11-06_to_2022-03-16_clean.csv.gzip\"\\ntweet_metrics = pd.read_csv(join(src, fname),\\n                 compression=\"gzip\",\\n                 usecols=[\"id\", \"retweet_count\",\\n                          \"reply_count\", \"like_count\", \"quote_count\"],\\n                dtype={\"id\":str})\\ntweet_metrics = tweet_metrics.drop_duplicates(subset=\"id\")\\n# merge the tweet metrics with the tweet data frame\\nurls = pd.merge(urls, tweet_metrics, how=\"left\", left_on=\"id\", right_on=\"id\")\\ndel tweet_metrics\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the public metrics information for the collected tweets\n",
    "'''\n",
    "src = \"../../data/tweets\"\n",
    "fname = \"combined_US_politician_twitter_timelines_2010-11-06_to_2022-03-16_clean.csv.gzip\"\n",
    "tweet_metrics = pd.read_csv(join(src, fname),\n",
    "                 compression=\"gzip\",\n",
    "                 usecols=[\"id\", \"retweet_count\",\n",
    "                          \"reply_count\", \"like_count\", \"quote_count\"],\n",
    "                dtype={\"id\":str})\n",
    "tweet_metrics = tweet_metrics.drop_duplicates(subset=\"id\")\n",
    "# merge the tweet metrics with the tweet data frame\n",
    "urls = pd.merge(urls, tweet_metrics, how=\"left\", left_on=\"id\", right_on=\"id\")\n",
    "del tweet_metrics\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d582d2-3902-4505-b3f2-912132db85ba",
   "metadata": {},
   "source": [
    "## Add unraveled URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0a307d6-8c0f-4f81-943f-2224dde5809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the list of originally shortened URLs with their expansions to their true\n",
    "# destination\n",
    "src = \"../../data/urls\"\n",
    "fname = \"US_unraveled_urls.csv.xz\"\n",
    "unraveled_urls = pd.read_csv(join(src, fname), compression=\"xz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99817340-0e3c-4ff3-bf3f-39e28d4c712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add URL information\n",
    "urls = pd.merge(urls, unraveled_urls, left_on=\"url\", right_on=\"url\", how=\"left\")\n",
    "\n",
    "# add indicator of whether the URL was originally shortened\n",
    "urls[\"shortened_url\"] = False\n",
    "urls.loc[urls[\"unraveled_url\"].dropna().index, \"shortened_url\"] = True\n",
    "\n",
    "# replace the shortened URL with the unraveled URL\n",
    "urls.loc[urls[\"unraveled_url\"].dropna().index, \"url\"] = \\\n",
    "    urls.loc[urls[\"unraveled_url\"].dropna().index, \"unraveled_url\"]\n",
    "urls = urls.drop(columns=[\"unraveled_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1eb618ca-2d99-4b12-b55d-34cba20edadf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found malformed URL https\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n"
     ]
    }
   ],
   "source": [
    "# extract the domain from the URL\n",
    "def extract_domain(url):\n",
    "    '''Given an ULR, extracts the domain name in the form XXXXX.YY'''\n",
    "    if url != url:\n",
    "        return np.nan\n",
    "    # reformat entries that have the domain after a general name in parantheses\n",
    "    if url.find('(') > 0:\n",
    "        url = url.split('(')[-1]\n",
    "        url = url.strip(')')\n",
    "    # trailing \"/\" and spaces\n",
    "    url = url.strip('/').strip()\n",
    "    # transform all domains to lowercase\n",
    "    url = url.lower()\n",
    "    # remove any white spaces\n",
    "    url = url.replace(' ', '')\n",
    "    # if present: remove the protocol\n",
    "    if url.startswith((\"http\", \"https\")):\n",
    "        try:\n",
    "            url = url.split('//')[1]\n",
    "        except IndexError:\n",
    "            print(f\"found malformed URL {url}\")\n",
    "            return np.nan\n",
    "    # remove \"www.\" \n",
    "    url = url.replace('www.', '')\n",
    "    url = url.split(\"/\")[0]\n",
    "    return url\n",
    "\n",
    "urls[\"domain\"] = urls[\"url\"].apply(extract_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beecaea6-0097-46bd-81ba-447d54aedbe7",
   "metadata": {},
   "source": [
    "## Add NewsGuard nutrition scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28bd17d-6a55-4776-bb34-082cf521baf6",
   "metadata": {},
   "source": [
    "Newsguard rating cutoff: 60 (see [description](https://www.newsguardtech.com/ratings/rating-process-criteria/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e27eb7a1-a3c0-4b14-8cd3-3b003a6d0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the nutrition labels\n",
    "src = \"../../data/utilities/\"\n",
    "fname = \"NewsGuard_labels.csv\"\n",
    "cols = [\"Domain\", \"Score\", \"Last Updated\"]\n",
    "NG_scores = pd.read_csv(join(src, fname), usecols=cols)\n",
    "# if more than one score exists for the same domain, keep the most recent one\n",
    "NG_scores = NG_scores.sort_values(by=[\"Domain\",\"Last Updated\"], ascending=False)\n",
    "NG_scores = NG_scores.drop_duplicates(subset=[\"Domain\"])\n",
    "NG_scores = NG_scores.rename(columns={\"Domain\":\"domain\", \"Score\":\"NG_score\"})\n",
    "NG_scores = NG_scores.drop(columns=[\"Last Updated\"])\n",
    "\n",
    "# threshold scores at various cutoffs to define untrustworthy domains\n",
    "NG_scores[\"NG_unreliable\"] = 0\n",
    "NG_scores.loc[NG_scores[NG_scores[\"NG_score\"] < 60].index, \"NG_unreliable\"] = 1\n",
    "\n",
    "# add the nutrition information to the tweet data table\n",
    "urls = pd.merge(urls, NG_scores,\n",
    "         left_on=\"domain\", right_on=\"domain\", how=\"left\")\n",
    "del NG_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddbcc04c-f946-4d8f-bace-70449904ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the list of all URLs with a NewsGuard score for article text straping\n",
    "dst = \"../../data/urls/\"\n",
    "fname = \"url_list_for_article_scraping.csv.gzip\"\n",
    "url_export = urls[[\"url\", \"domain\", \"NG_score\"]].copy()\n",
    "url_export = url_export.drop_duplicates().dropna(subset=[\"url\"])\n",
    "url_export.to_csv(join(dst, fname), index=False, compression=\"gzip\")\n",
    "del url_export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc4e71e-2b3e-4443-a9a7-aef9deef614a",
   "metadata": {},
   "source": [
    "## Add alternative trustworthiness labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28a4a1fe-f8c2-4dd9-a93d-db0c7cadc131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the list of independently compiled trustworthiness labels for \n",
    "# news sources\n",
    "src = \"../../data/utilities\"\n",
    "fname = \"independent_labels.csv\"\n",
    "alt_labels = pd.read_csv(join(src, fname))\n",
    "alt_labels = alt_labels.rename(columns = {\n",
    "    \"type\":\"independent_unreliable\", \n",
    "    \"url\":\"domain\"})\n",
    "\n",
    "# convert reliability labels to binary\n",
    "alt_labels[\"independent_unreliable\"] = alt_labels[\"independent_unreliable\"]\\\n",
    "    .replace({\"reliable\":0, \"unreliable\":1})\n",
    "\n",
    "# merge with the tweet data table\n",
    "urls = pd.merge(urls, alt_labels[[\"accuracy\", \"transparency\", \n",
    "        \"independent_unreliable\", \"domain\"]], how=\"left\", left_on=\"domain\",\n",
    "         right_on=\"domain\")\n",
    "del alt_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ae920e-36fa-413c-b46c-e005aa0c64b4",
   "metadata": {},
   "source": [
    "## Add truth seeking & belief speaking scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fdaf3065-2c0d-4d15-9c7a-7767ac5cb0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the word matching counts for belief-speaking and truth-seeking\n",
    "src = \"../../data/tweets\"\n",
    "#fname = \"combined_US_politician_twitter_timelines_2010-11-06_to_2022-03-16_honesty_component_labels.csv.gzip\"\n",
    "fname = \"combined_US_politician_twitter_timelines_2010-11-06_to_2022-03-16_p0.05_swapped_label_wn_word_definition_sbert_avgsim_nolemma.csv.gzip\"\n",
    "cols = [\"id\", \"belief_count\", \"truth_count\"]\n",
    "honesty_labels = pd.read_csv(\n",
    "    join(src, fname),\n",
    "    usecols=cols,\n",
    "    dtype={\"id\":str}, \n",
    "    compression=\"gzip\"\n",
    ")\n",
    "honesty_labels[\"id\"] = honesty_labels[\"id\"].apply(lambda x: x.replace('\"', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6ae406b9-1806-4509-af1b-fc439d47d681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# belief-speaking and truth-seeking labels for each tweet are assigned based on\n",
    "# the majority of words matching to one of the two components. If there is a \n",
    "# tie, the tweet is assigned to both components this results in\n",
    "# 1602160 neutral tweets\n",
    "# 91280 unambiguous belief tweets\n",
    "# 106355 unambiguous truth tweets\n",
    "# 7050 ties with count > 0 including 86 ties with count > 1 and \n",
    "# 1 tie with count > 2\n",
    "honesty_labels[\"belief\"] = 0\n",
    "honesty_labels[\"truth\"] = 0\n",
    "honesty_labels[\"neutral\"] = 0\n",
    "\n",
    "# unambigous majority votes\n",
    "honesty_labels.loc[honesty_labels[honesty_labels[\"belief_count\"] > \\\n",
    "                    honesty_labels[\"truth_count\"]].index, \"belief\"] = 1\n",
    "honesty_labels.loc[honesty_labels[honesty_labels[\"truth_count\"] > \\\n",
    "                    honesty_labels[\"belief_count\"]].index, \"truth\"] = 1\n",
    "\n",
    "# ties\n",
    "honesty_labels.loc[honesty_labels[(honesty_labels[\"truth_count\"] == \\\n",
    "                    honesty_labels[\"belief_count\"]) & \\\n",
    "                    (honesty_labels[\"truth_count\"] > 0)].index, \"truth\"] = 1\n",
    "honesty_labels.loc[honesty_labels[(honesty_labels[\"truth_count\"] == \\\n",
    "                    honesty_labels[\"belief_count\"]) &\\\n",
    "                    (honesty_labels[\"truth_count\"] > 0)].index, \"belief\"] = 1\n",
    "\n",
    "# neutral\n",
    "honesty_labels.loc[honesty_labels[(honesty_labels[\"truth_count\"] == 0) & \\\n",
    "                    (honesty_labels[\"belief_count\"] == 0)].index, \"neutral\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d1c321c2-4fbf-4fbc-9f00-734c42db8838",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = pd.merge(honesty_labels[[\"id\", \"belief\", \"truth\", \"neutral\"]], \n",
    "         urls, how=\"right\", left_on=\"id\", right_on=\"id\")\n",
    "del honesty_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615e8610-dd79-4384-a4c9-962ab6628cf1",
   "metadata": {},
   "source": [
    "## Add party affiliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8baa890d-8c48-4c33-98ea-e7655f053d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../data/utilities\"\n",
    "fname = \"party_affiliations_complete.csv\"\n",
    "party_affiliation = pd.read_csv(join(src, fname), dtype={\"author_id\":str})\n",
    "urls = pd.merge(urls, party_affiliation, how=\"left\", left_on=\"author_id\",\n",
    "    right_on=\"author_id\")\n",
    "del party_affiliation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854407a9-c578-4442-a0de-b671232aaa3a",
   "metadata": {},
   "source": [
    "# Create a tweet data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "58b04781-9dad-4f95-bcbf-67ae7fedee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the current \"url\" data frame contains one row per URL, i.e. the same\n",
    "# tweet can be present more than once. To calculate the share of tweets with\n",
    "# unreliable information, we first calculate the mean NewsGuard score (and \n",
    "# mean accuracy and transparency) per tweet by averaging over all scores \n",
    "# of URLs that are present in a given tweet and then assigning \"fishy\" and\n",
    "# \"unreliable\" labels on the tweet level\n",
    "\n",
    "# columns that are defined on the tweet level\n",
    "tweet_cols = [\"id\", \"belief\", \"truth\", \"neutral\", \"author_id\",\n",
    "              \"created_at\", \"retweeted\", \"quoted\", \"reply\", \"has_url\",\n",
    "              \"handle\", \"name\", \"party\"]\n",
    "tweets = urls[tweet_cols].drop_duplicates(subset=[\"id\"]).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca01b4-9486-42e8-9b88-d3eb713989d9",
   "metadata": {},
   "source": [
    "## Add LIWC scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "11fe50d8-a937-4a04-aa14-786ef8ed7d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../data/tweets\"\n",
    "fname = \"combined_US_politician_twitter_timelines_2010-11-06_to_2022-03-16_clean_mask_LIWC.csv.gzip\"\n",
    "cols = [\"id\", \"WC\", \"Analytic\", \"Authentic\", \"moral\", \"emo_pos\", \"emo_neg\"]\n",
    "LIWC_scores = pd.read_csv(\n",
    "    join(src, fname), \n",
    "    compression=\"gzip\",\n",
    "    usecols=cols,\n",
    "    dtype={\"id\":str},\n",
    ")\n",
    "LIWC_scores = LIWC_scores.rename(columns={\n",
    "    \"WC\":\"word_count\",\n",
    "    \"Analytic\":\"LIWC_analytic\",\n",
    "    \"Authentic\":\"LIWC_authentic\",\n",
    "    \"moral\":\"LIWC_moral\",\n",
    "    \"emo_pos\":\"LIWC_emo_pos\",\n",
    "    \"emo_neg\":\"LIWC_emo_neg\"\n",
    "})\n",
    "tweets = pd.merge(tweets, LIWC_scores, how=\"left\", left_on=\"id\", right_on=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a6213b-1870-4d10-b0eb-e29437d76fde",
   "metadata": {},
   "source": [
    "## Calculate average NewsGuard score and misinfo components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a0fc3332-0793-444f-8764-ae7578b6b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_scores = urls[[\"id\", \"NG_score\", \"transparency\", \"accuracy\"]]\\\n",
    "    .groupby(\"id\")\\\n",
    "    .agg(\"mean\")\n",
    "\n",
    "average_scores[\"NG_unreliable\"] = np.nan\n",
    "average_scores.loc[average_scores[\\\n",
    "            average_scores[\"NG_score\"] < 60].index, \"NG_unreliable\"] = 1\n",
    "average_scores.loc[average_scores[\\\n",
    "            average_scores[\"NG_score\"] >= 60].index, \"NG_unreliable\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b490d1c0-59ab-4342-922c-fce0572d5d5e",
   "metadata": {},
   "source": [
    "## Calculate average accuracy & transparency score and unreliable domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9f70b5d3-ce28-4609-a8de-52feadeeaefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_scores[\"independent_unreliable\"] = np.nan\n",
    "# original definition: sources with transparency = 1 are unreliable\n",
    "# since transparency can have non-integer values after averaging, we decide\n",
    "# to label tweets with an average domain transparency value of links of\n",
    "# <= 1.5 as \"unreliable\", since that means that the majority of domains \n",
    "# linked to in the tweet are unreliable. If one domain with transparency 1\n",
    "# and one domain with transparency 2 are linked, the tweet is unreliable\n",
    "average_scores.loc[average_scores[\\\n",
    "            average_scores[\"transparency\"] <= 1.5].index, \"independent_unreliable\"] = 1\n",
    "average_scores.loc[average_scores[\\\n",
    "            average_scores[\"transparency\"] > 1.5].index, \"independent_unreliable\"] = 0\n",
    "# original defintion: sources with accuracy = 1 or 2 are unreliable\n",
    "# since accuracy can have non-integer values after averaging, we decide to\n",
    "# label tweets with an average domain accuracy value of links of <= 2.5 as\n",
    "# \"unreliable\", since that means that the majority of domains linked to in \n",
    "# the tweet are unreliable. If one domain with accuracy 2 and one domain \n",
    "# with accuracy 3 are linked, the tweet is unreliable.\n",
    "average_scores.loc[average_scores[\\\n",
    "            average_scores[\"accuracy\"] <= 2.5].index, \"independent_unreliable\"] = 1\n",
    "average_scores.loc[average_scores[\\\n",
    "            average_scores[\"accuracy\"] > 2.5].index, \"independent_unreliable\"] = 0\n",
    "\n",
    "tweets = pd.merge(tweets, average_scores, how=\"left\", left_on=\"id\", right_on=\"id\")\n",
    "del average_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcce382-f6bf-4ad7-9b57-08ef20082a65",
   "metadata": {},
   "source": [
    "# Create a user data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc2de824-8f53-4876-abf6-01056ffdc0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = tweets[[\"author_id\", \"handle\", \"name\", \"party\", \"id\"]]\\\n",
    "    .groupby([\"author_id\", \"handle\", \"name\", \"party\"])\\\n",
    "    .agg(\"count\")\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns={\"id\":\"N_tweets\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0295d1f2-3815-4634-a192-dfa5166f6aa5",
   "metadata": {},
   "source": [
    "## Add account stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd97affd-963c-4ed5-9fcb-74a04207b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../data/users\"\n",
    "fname = \"US_politician_twitter_accounts_clean.csv\"\n",
    "cols = [\"followers_count\", \"following_count\", \"tweet_count\", \"created_at\", \n",
    "        \"author_id\"]\n",
    "account_stats = pd.read_csv(\n",
    "    join(src, fname),\n",
    "    parse_dates=[\"created_at\"],\n",
    "    usecols=cols,\n",
    "    dtype={\"author_id\":str}\n",
    ")\n",
    "\n",
    "users = pd.merge(users, account_stats, how=\"left\", left_on=\"author_id\", right_on=\"author_id\")\n",
    "del account_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261401b6-eb61-4495-9bdb-a12570397fa0",
   "metadata": {},
   "source": [
    "## Add Congress information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e69ec38a-a3d1-4452-b740-ab537fbe7159",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../data/users/clean\"\n",
    "fname = \"congress-member-twitter-handles_114-117.csv\"\n",
    "congress_twitter_handles = pd.read_csv(join(src, fname))\n",
    "congress_twitter_handles = congress_twitter_handles\\\n",
    "    .sort_values(by=\"congress\", ascending=False)\\\n",
    "    .drop_duplicates(subset=\"handle\")\\\n",
    "    .reset_index(drop=True)\n",
    "\n",
    "users = pd.merge(users, congress_twitter_handles, how=\"left\", left_on=\"handle\",\n",
    "                 right_on=\"handle\")\n",
    "del congress_twitter_handles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11974ba2-c6c1-42fa-9d99-bc1f1b949fd6",
   "metadata": {},
   "source": [
    "## Add share of untrustworthy domains (NewsGuard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a056d98f-6208-40d2-bb48-68e7861ac7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>NG_unreliable_sum</th>\n",
       "      <th>NG_unreliable_count</th>\n",
       "      <th>NG_unreliable_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1009269193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>221</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1011053278304592000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author_id  NG_unreliable_sum  NG_unreliable_count  \\\n",
       "0           1009269193                0.0                  221   \n",
       "1  1011053278304592000                0.0                    0   \n",
       "\n",
       "   NG_unreliable_share  \n",
       "0                  0.0  \n",
       "1                  NaN  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"author_id\", \"NG_unreliable\"]\n",
    "unreliable_user_count = tweets[tweets[\"retweeted\"] == False][cols]\\\n",
    "    .groupby(\"author_id\")\\\n",
    "    .agg([\"sum\", \"count\"])\n",
    "\n",
    "unreliable_user_count[\"NG_unreliable_share\"] = \\\n",
    "    unreliable_user_count[\"NG_unreliable\"][\"sum\"] / \\\n",
    "    unreliable_user_count[\"NG_unreliable\"][\"count\"]\n",
    "    \n",
    "# flatten the hierarchical indices\n",
    "unreliable_user_count = unreliable_user_count.reset_index()\n",
    "unreliable_user_count.columns = ['_'.join(col).strip(\"_\") \\\n",
    "                            for col in unreliable_user_count.columns.values]\n",
    "\n",
    "unreliable_user_count.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e070f70f-4543-4858-be3e-cf2bded68125",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"NG_unreliable_share\", \"author_id\"]\n",
    "users = pd.merge(\n",
    "    users, \n",
    "    unreliable_user_count[cols],\n",
    "    how=\"left\",\n",
    "    left_on=\"author_id\",\n",
    "    right_on=\"author_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a1daea-e29c-4328-b1c2-05f8e18f3a0b",
   "metadata": {},
   "source": [
    "## Add average NewsGuard score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e7b53b46-f10e-4ef0-93e9-e86325f82211",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_NG_scores = tweets[tweets[\"retweeted\"] == False][[\"author_id\", \"NG_score\"]]\\\n",
    "    .groupby(\"author_id\")\\\n",
    "    .mean()\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns={\"NG_score\":\"NG_score_mean\"})\n",
    "users = pd.merge(\n",
    "    users, \n",
    "    average_NG_scores, \n",
    "    how=\"left\", \n",
    "    left_on=\"author_id\", \n",
    "    right_on=\"author_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6307c56-adfd-4d83-b111-9bb2e8461225",
   "metadata": {},
   "source": [
    "## Add average accuracy & transparency score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d17fb525-9a04-4192-8d16-7729d7041660",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_accuracy_transparency = tweets[tweets[\"retweeted\"] == False]\\\n",
    "    [[\"author_id\", \"accuracy\", \"transparency\"]]\\\n",
    "    .groupby(\"author_id\")\\\n",
    "    .mean()\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns={\n",
    "        \"accuracy\":\"accuracy_mean\",\n",
    "        \"transparency\":\"transparency_mean\"\n",
    "    })\n",
    "users = pd.merge(\n",
    "    users, \n",
    "    average_accuracy_transparency, \n",
    "    how=\"left\", \n",
    "    left_on=\"author_id\", \n",
    "    right_on=\"author_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977a4007-d097-44f5-a790-14bc31711dec",
   "metadata": {},
   "source": [
    "## Add share of unstrustworthy domains (independent list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "40b07a89-36ca-4677-a73b-e853a1a20029",
   "metadata": {},
   "outputs": [],
   "source": [
    "unreliable_user_count = tweets[tweets[\"retweeted\"] == False]\\\n",
    "    [[\"author_id\", \"independent_unreliable\"]]\\\n",
    "    .groupby(\"author_id\")\\\n",
    "    .agg([\"sum\", \"count\"])\n",
    "\n",
    "unreliable_user_count[\"independent_unreliable_share\"] = \\\n",
    "    unreliable_user_count[\"independent_unreliable\"][\"sum\"] / \\\n",
    "    unreliable_user_count[\"independent_unreliable\"][\"count\"]\n",
    "    \n",
    "# flatten the hierarchical indices\n",
    "unreliable_user_count = unreliable_user_count.reset_index()\n",
    "unreliable_user_count.columns = ['_'.join(col).strip(\"_\") \\\n",
    "                            for col in unreliable_user_count.columns.values]\n",
    "\n",
    "users = pd.merge(\n",
    "    users, \n",
    "    unreliable_user_count[[\"author_id\", \"independent_unreliable_share\"]],\n",
    "    how=\"left\", \n",
    "    left_on=\"author_id\", \n",
    "    right_on=\"author_id\"\n",
    ")\n",
    "del unreliable_user_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2a79f8-ac7c-4e11-b20a-7e9103024582",
   "metadata": {},
   "source": [
    "## Add share of belief-speaking and truth-seeking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2be1701a-4889-4c8e-9232-8af3ffa360f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "honesty_tweets = tweets[tweets[\"retweeted\"] == False]\\\n",
    "    [[\"author_id\", \"belief\", \"truth\", \"created_at\"]]\\\n",
    "    .dropna(subset=[\"belief\", \"truth\"])\\\n",
    "    .copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3ca6315e-3f99-4205-a192-f3cb2f7d876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all honesty component tweets\n",
    "honesty_label_count = honesty_tweets[[\"author_id\", \"belief\", \"truth\"]]\\\n",
    "    .groupby(\"author_id\")\\\n",
    "    .agg([\"sum\", \"count\"])\n",
    "\n",
    "for col in [\"belief\", \"truth\"]:\n",
    "    honesty_label_count[f\"{col}_share\"] = honesty_label_count[col][\"sum\"] / \\\n",
    "    honesty_label_count[col][\"count\"]\n",
    "    \n",
    "honesty_label_count.columns = ['_'.join(col).strip(\"_\") \\\n",
    "                            for col in honesty_label_count.columns.values]\n",
    "honesty_label_count = honesty_label_count.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f78321f2-4713-4c72-8dad-7f0c1699df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "honesty_tweets = honesty_tweets.set_index(\"created_at\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "425a85c9-0fcd-482c-8885-c6fc2ffe6db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only first 4 years\n",
    "honesty_label_count_first = honesty_tweets[honesty_tweets.index.year <= 2013]\\\n",
    "    .groupby(\"author_id\")\\\n",
    "    .agg([\"sum\", \"count\"])\n",
    "\n",
    "for col in [\"belief\", \"truth\"]:\n",
    "    honesty_label_count_first[f\"{col}_share_2010_to_2013\"] = \\\n",
    "        honesty_label_count_first[col][\"sum\"] / \\\n",
    "        honesty_label_count_first[col][\"count\"]\n",
    "    \n",
    "honesty_label_count_first.columns = ['_'.join(col).strip(\"_\") \\\n",
    "                            for col in honesty_label_count_first.columns.values]\n",
    "honesty_label_count_first = honesty_label_count_first.reset_index()\n",
    "cols = [\"belief_sum\", \"belief_count\", \"truth_sum\", \"truth_count\"]\n",
    "honesty_label_count_first = honesty_label_count_first\\\n",
    "    .rename(columns={col:col + \"_2010_to_2013\" for col in cols}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d6e0d952-d625-48e1-b7bf-25211b9cfda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only last 4 years\n",
    "honesty_label_count_last = honesty_tweets[honesty_tweets.index.year >= 2019]\\\n",
    "    [[\"author_id\", \"belief\", \"truth\"]]\\\n",
    "    .groupby(\"author_id\")\\\n",
    "    .agg([\"sum\", \"count\"])\n",
    "\n",
    "for col in [\"belief\", \"truth\"]:\n",
    "    honesty_label_count_last[f\"{col}_share_2019_to_2022\"] = \\\n",
    "    honesty_label_count_last[col][\"sum\"] / \\\n",
    "    honesty_label_count_last[col][\"count\"]\n",
    "    \n",
    "honesty_label_count_last.columns = ['_'.join(col).strip(\"_\") \\\n",
    "                            for col in honesty_label_count_last.columns.values]\n",
    "honesty_label_count_last = honesty_label_count_last.reset_index()\n",
    "cols = [\"belief_sum\", \"belief_count\", \"truth_sum\", \"truth_count\"]\n",
    "honesty_label_count_last = honesty_label_count_last\\\n",
    "    .rename(columns={col:col + \"_2019_to_2022\" for col in cols}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "075e1209-11b5-46d5-bb87-beeb11251365",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users.merge(honesty_label_count[[\"author_id\", \"belief_share\", \n",
    "                    \"truth_share\"]], how=\"left\", left_on=\"author_id\", \n",
    "                    right_on=\"author_id\")\n",
    "del honesty_label_count\n",
    "\n",
    "cols = [\"author_id\", \"belief_share_2010_to_2013\",\"truth_share_2010_to_2013\"]\n",
    "users = users.merge(\n",
    "    honesty_label_count_first[cols],\n",
    "    how=\"left\",\n",
    "    left_on=\"author_id\", \n",
    "    right_on=\"author_id\"\n",
    ")\n",
    "del honesty_label_count_first\n",
    "\n",
    "cols = [\"author_id\",\"belief_share_2019_to_2022\", \"truth_share_2019_to_2022\"]\n",
    "users = users.merge(\n",
    "    honesty_label_count_last[cols],\n",
    "    how=\"left\",\n",
    "    left_on=\"author_id\", \n",
    "    right_on=\"author_id\"\n",
    ")\n",
    "del honesty_label_count_last"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624afd34-bcd0-434e-b756-61453b28276f",
   "metadata": {},
   "source": [
    "## Add share of neutral tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "edf0e7f8-08b7-4237-ba3c-6933667519d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "honesty_tweets = honesty_tweets.reset_index()\n",
    "neutral_count = honesty_tweets[honesty_tweets[[\"belief\", \"truth\"]]\\\n",
    "    .sum(axis=1) == 0][[\"author_id\", \"created_at\"]]\\\n",
    "    .groupby(\"author_id\")\\\n",
    "    .agg(\"count\")\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns={\"created_at\":\"neutral_count\"})\n",
    "\n",
    "users = pd.merge(\n",
    "    users, \n",
    "    neutral_count, \n",
    "    how=\"left\", \n",
    "    left_on=\"author_id\",\n",
    "    right_on=\"author_id\"\n",
    ")#.dropna(subset=[\"neutral_count\"])\n",
    "users[\"neutral_share\"] = users[\"neutral_count\"] / users[\"N_tweets\"]\n",
    "users = users.drop(columns=[\"neutral_count\"])\n",
    "del honesty_tweets\n",
    "del neutral_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4d5df9-59f6-42d8-85e7-8c3702603805",
   "metadata": {},
   "source": [
    "## Add ideology scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e77d54eb-c6c5-4ca6-9980-9dec16b0b719",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../data/utilities\"\n",
    "fname = \"govtrack-stats-{}-{}-ideology.csv\"\n",
    "ideology_scores = pd.DataFrame()\n",
    "for year in range(2013, 2021):\n",
    "    for chamber in [\"house\", \"senate\"]:\n",
    "        tmp = pd.read_csv(join(src, \"ideology_scores\",\n",
    "                               fname.format(year, chamber)))\n",
    "        tmp[\"year\"] = year\n",
    "        tmp[\"name\"] = tmp[\"name\"].apply(lambda x: x.replace(\"b'\", \"\"))\n",
    "        tmp[\"name\"] = tmp[\"name\"].apply(lambda x: x.replace(\"'\", \"\").lower())\n",
    "        ideology_scores = pd.concat([ideology_scores, tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4225918e-470e-4139-a980-d1c97587e40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# match politician Twitter account names to govtrack politician names\n",
    "\n",
    "# a single politician can have at maximum 8 entries for 8 different years\n",
    "# 2013 to 2020\n",
    "counts = ideology_scores[\"name\"].value_counts()\n",
    "unique_names = list(counts[counts <= 8].index)\n",
    "\n",
    "unique_scores = ideology_scores[ideology_scores[\"name\"].isin(unique_names)]\\\n",
    "    .sort_values(by=\"year\", ascending=False)\\\n",
    "    .drop_duplicates(subset=[\"name\"])\\\n",
    "    .set_index(\"name\")\n",
    "unique_names = list(set(unique_scores.index))\n",
    "\n",
    "def match_score(account_name):\n",
    "    '''Matches govtrack politician names to Twitter account names.'''\n",
    "    if account_name == account_name:\n",
    "        account_name = set(account_name.lower().split(\" \"))\n",
    "        for name in unique_names:\n",
    "            # hard matching: if the govtrack name string is completely included\n",
    "            # in the Twitter account name string, record a match\n",
    "            if name in account_name:\n",
    "                return unique_scores.loc[name][\"id\"]\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "users[\"ideology_score_id\"] = users[\"name\"].apply(match_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "28e6f597-1240-4040-a814-8fab957ba0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add hand-matched missing scores\n",
    "src = \"../../data/utilities\"\n",
    "fname = \"missing_govtrack_ideology_scores.csv\"\n",
    "missing_scores = pd.read_csv(join(src, fname))\n",
    "missing_scores = {row[\"handle\"]:row[\"ideology_score_id\"] \\\n",
    "                  for i, row in missing_scores.iterrows()}\n",
    "\n",
    "# index on the handle since this seems to be the most consistent index between\n",
    "# the two datasets\n",
    "users = users.set_index(\"handle\")\n",
    "for handle, score_id in missing_scores.items():\n",
    "    if handle in users.index:\n",
    "        users.loc[handle, \"ideology_score_id\"] = score_id\n",
    "users = users.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "970b5966-4348-4e4a-b270-9f7b87177fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for many accounts, there is more than one ideology score since they were \n",
    "# active over many years. We calculate the mean, std and count of the ideology\n",
    "# score for each user and add this information to the user_df\n",
    "ideology_scores_agg = ideology_scores[[\"id\", \"ideology\"]]\\\n",
    "    .groupby(\"id\")\\\n",
    "    .agg([\"mean\", \"std\", \"count\"])\n",
    "ideology_scores_agg = ideology_scores_agg.reset_index()\n",
    "ideology_scores_agg.columns = ['_'.join(col).strip(\"_\") \\\n",
    "                            for col in ideology_scores_agg.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "74895b07-eee3-4235-8818-e5c9e2f799e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users.merge(ideology_scores_agg, how=\"left\", \n",
    "                      left_on=\"ideology_score_id\", right_on=\"id\")\n",
    "del ideology_scores\n",
    "del ideology_scores_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e52b7e7-4487-4e38-a701-e572c43930dc",
   "metadata": {},
   "source": [
    "## Add Politifact scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cc203c1d-6d46-413d-a1b5-2fb3a1e88b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../data/utilities\"\n",
    "fname = \"misinfo_score_politifact.csv\"\n",
    "pf_scores = pd.read_csv(join(src, fname), \n",
    "        usecols=[\"pf_score\", \"elite_account\"])\\\n",
    "    .rename(columns={\"elite_account\":\"handle\"})\n",
    "\n",
    "users = pd.merge(users, pf_scores, how=\"left\", left_on=\"handle\", right_on=\"handle\")\n",
    "del pf_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4c0265-fecb-4048-bd50-802e4eeb3177",
   "metadata": {},
   "source": [
    "# Data exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7f0fcc5e-1982-4323-801c-6614c8250d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = \"../../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "38c29338-6b28-41e7-b93c-c2ae005d9351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL data frame\n",
    "fname = \"US_politician_URLs_2010-11-06_to_2022-03-16.csv.gzip\"\n",
    "urls = urls[urls[\"has_url\"] == True]\n",
    "urls = urls.drop(columns=[\"url\", \"domain\", \"status_code\", \"handle\", \"name\", \"has_url\"])\n",
    "urls.to_csv(join(dst, \"urls\", fname), index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5d12e8d4-6651-4dc0-988a-fca8d50be901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user data frame\n",
    "fname = \"US_politician_accounts_2010-11-06_to_2022-03-16.csv\"\n",
    "users = users.drop(columns=[\"ideology_score_id\", \"id\"])\n",
    "users.to_csv(join(dst, \"users\", fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e2e6b858-14f8-40d0-bdc4-7ad842836fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet data frame\n",
    "fname = \"US_politician_tweets_2010-11-06_to_2022-03-16.csv.gzip\"\n",
    "tweets = tweets.drop(columns=[\"handle\", \"name\"])\n",
    "tweets.to_csv(join(dst, \"tweets\", fname), index=False, compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
