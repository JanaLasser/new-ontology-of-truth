{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e2a9c7-676a-4f9c-b3b1-cab3814a017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# author: Jana Lasser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7df11ea-dde5-4bde-992c-5f0b4bb42cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06354d31-f480-447f-95a2-d653305a5770",
   "metadata": {},
   "source": [
    "# Get user timelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0056b1a8-5aff-40f9-aa07-d2fa9bc136c5",
   "metadata": {},
   "source": [
    "**Note**: on 2023-02-11 we discovered that we were using the twarc2 timelines command without the --use-search parameter. This resulted in us getting only the last 3200 tweets from every account. We repeated the data collection on 2023-02-11 and 2023-02-12 to retrieve the missing tweets. The old data is in the folder ../data_2022-02-11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "694c6433-18e7-4224-b311-cee81a254197",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../data/users\"\n",
    "fname = \"US_politician_twitter_accounts_clean.csv\"\n",
    "users = pd.read_csv(Path(src, fname), dtype={\"author_id\":str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "346446ce-2869-417a-a2b6-4d26d2b6bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the list of accounts into batches\n",
    "dst = \"../../data/users\"\n",
    "if not os.path.exists(Path(dst, \"user_batches\")): os.mkdir(Path(dst, \"user_batches\"))\n",
    "N_keys = 11\n",
    "batch_size = int(len(users) / N_keys)\n",
    "for i in range(N_keys):\n",
    "    batch = users[\"author_id\"][i * batch_size : (i+ 1) * batch_size]\n",
    "    np.savetxt(Path(dst, \"user_batches\", f\"US_politician_twitter_accounts_batch_{i}.txt\"), \n",
    "               batch, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aec6d54-374d-4924-a2d4-80918fc3ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the account ID batches to the server\n",
    "! rsync -avze ssh ../../data/users/user_batches/US_politician_twitter_accounts_batch_* jlasser@medea:/data/honesty/corpora/Twitter/US_politician_twitter_accounts_ontology/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb037d28-c8ae-473c-8ed7-9f8b52c6bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commands on medea to download the user timelines for each batch using twarc2\n",
    "cd /data/honesty/corpora/Twitter/US_politician_twitter_accounts_ontology\n",
    "twarc2 timelines --use-search --no-context-annotations --start-time 2010-11-06 --end-time 2023-02-11 US_politician_twitter_accounts_batch_0.txt ../US_politician_timelines_ontology_update/US_politician_twitter_accounts_batch_0.jsonl\n",
    "twarc2 timelines --use-search --no-context-annotations --start-time 2010-11-06 --end-time 2023-02-11 US_politician_twitter_accounts_batch_1.txt ../US_politician_timelines_ontology_update/US_politician_twitter_accounts_batch_1.jsonl\n",
    "twarc2 timelines --use-search --no-context-annotations --start-time 2010-11-06 --end-time 2023-02-11 US_politician_twitter_accounts_batch_2.txt ../US_politician_timelines_ontology_update/US_politician_twitter_accounts_batch_2.jsonl\n",
    "twarc2 timelines --use-search --no-context-annotations --start-time 2010-11-06 --end-time 2023-02-11 US_politician_twitter_accounts_batch_3.txt ../US_politician_timelines_ontology_update/US_politician_twitter_accounts_batch_3.jsonl\n",
    "twarc2 timelines --use-search --no-context-annotations --start-time 2010-11-06 --end-time 2023-02-11 US_politician_twitter_accounts_batch_4.txt ../US_politician_timelines_ontology_update/US_politician_twitter_accounts_batch_4.jsonl\n",
    "twarc2 timelines --use-search --no-context-annotations --start-time 2010-11-06 --end-time 2023-02-11 US_politician_twitter_accounts_batch_5.txt ../US_politician_timelines_ontology_update/US_politician_twitter_accounts_batch_5.jsonl\n",
    "twarc2 timelines --use-search --no-context-annotations --start-time 2010-11-06 --end-time 2023-02-11 US_politician_twitter_accounts_batch_6.txt ../US_politician_timelines_ontology_update/US_politician_twitter_accounts_batch_6.jsonl\n",
    "twarc2 timelines --use-search --no-context-annotations --start-time 2010-11-06 --end-time 2023-02-11 US_politician_twitter_accounts_batch_7.txt ../US_politician_timelines_ontology_update/US_politician_twitter_accounts_batch_7.jsonl\n",
    "twarc2 timelines --use-search --no-context-annotations --start-time 2010-11-06 --end-time 2023-02-11 US_politician_twitter_accounts_batch_8.txt ../US_politician_timelines_ontology_update/US_politician_twitter_accounts_batch_8.jsonl\n",
    "twarc2 timelines --use-search --no-context-annotations --start-time 2010-11-06 --end-time 2023-02-11 US_politician_twitter_accounts_batch_9.txt ../US_politician_timelines_ontology_update/US_politician_twitter_accounts_batch_9.jsonl\n",
    "\n",
    "# additional accounts of the 118th congress\n",
    "twarc2 timelines --use-search --no-context-annotations --start-time 2010-11-06 --end-time 2023-02-11 US_politician_twitter_accounts_batch_10.txt ../US_politician_timelines_ontology_update/US_politician_twitter_accounts_batch_10.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6558b1-89c9-4c27-bc6d-12fc4e3fb18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commands on the server to convert the raw .json files to .csv\n",
    "cd ../US_politician_timelines_ontology_update\n",
    "twarc2 csv --extra-input-columns \"public_metrics.impression_count,edit_history_tweet_ids\" --input-data-type tweets US_politician_twitter_accounts_batch_0.jsonl US_politician_twitter_accounts_batch_0.csv\n",
    "twarc2 csv --extra-input-columns \"public_metrics.impression_count,edit_history_tweet_ids\" --input-data-type tweets US_politician_twitter_accounts_batch_1.jsonl US_politician_twitter_accounts_batch_1.csv\n",
    "twarc2 csv --extra-input-columns \"public_metrics.impression_count,edit_history_tweet_ids\" --input-data-type tweets US_politician_twitter_accounts_batch_2.jsonl US_politician_twitter_accounts_batch_2.csv\n",
    "twarc2 csv --extra-input-columns \"public_metrics.impression_count,edit_history_tweet_ids\" --input-data-type tweets US_politician_twitter_accounts_batch_3.jsonl US_politician_twitter_accounts_batch_3.csv\n",
    "twarc2 csv --extra-input-columns \"public_metrics.impression_count,edit_history_tweet_ids\" --input-data-type tweets US_politician_twitter_accounts_batch_4.jsonl US_politician_twitter_accounts_batch_4.csv\n",
    "twarc2 csv --extra-input-columns \"public_metrics.impression_count,edit_history_tweet_ids\" --input-data-type tweets US_politician_twitter_accounts_batch_5.jsonl US_politician_twitter_accounts_batch_5.csv\n",
    "twarc2 csv --extra-input-columns \"public_metrics.impression_count,edit_history_tweet_ids\" --input-data-type tweets US_politician_twitter_accounts_batch_6.jsonl US_politician_twitter_accounts_batch_6.csv\n",
    "twarc2 csv --extra-input-columns \"public_metrics.impression_count,edit_history_tweet_ids\" --input-data-type tweets US_politician_twitter_accounts_batch_7.jsonl US_politician_twitter_accounts_batch_7.csv\n",
    "twarc2 csv --extra-input-columns \"public_metrics.impression_count,edit_history_tweet_ids\" --input-data-type tweets US_politician_twitter_accounts_batch_8.jsonl US_politician_twitter_accounts_batch_8.csv\n",
    "twarc2 csv --extra-input-columns \"public_metrics.impression_count,edit_history_tweet_ids\" --input-data-type tweets US_politician_twitter_accounts_batch_9.jsonl US_politician_twitter_accounts_batch_9.csv\n",
    "\n",
    "# additional accounts of the 118th congress\n",
    "twarc2 csv --extra-input-columns \"public_metrics.impression_count,edit_history_tweet_ids\" --input-data-type tweets US_politician_twitter_accounts_batch_10.jsonl US_politician_twitter_accounts_batch_10.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679049cf-f8f8-4451-a7a4-95f02e3ea64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commands on the server to compress the .csv files\n",
    "xz -v US_politician_twitter_accounts_batch_0.csv\n",
    "xz -v US_politician_twitter_accounts_batch_1.csv\n",
    "xz -v US_politician_twitter_accounts_batch_2.csv\n",
    "xz -v US_politician_twitter_accounts_batch_3.csv\n",
    "xz -v US_politician_twitter_accounts_batch_4.csv\n",
    "xz -v US_politician_twitter_accounts_batch_5.csv\n",
    "xz -v US_politician_twitter_accounts_batch_6.csv\n",
    "xz -v US_politician_twitter_accounts_batch_7.csv\n",
    "xz -v US_politician_twitter_accounts_batch_8.csv\n",
    "xz -v US_politician_twitter_accounts_batch_9.csv\n",
    "\n",
    "# additional accounts of the 118th congress\n",
    "xz -v US_politician_twitter_accounts_batch_10.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2103399e-4f47-42d5-ba3b-83a170749505",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ../../data/tweets/timeline_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "710abd6f-48c2-47a8-971f-9c8c25b5bf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "receiving incremental file list\n",
      "US_politician_twitter_accounts_batch_10.csv.xz\n",
      "\n",
      "sent 43 bytes  received 35,404,389 bytes  7,867,651.56 bytes/sec\n",
      "total size is 1,133,057,136  speedup is 32.00\n"
     ]
    }
   ],
   "source": [
    "# download the .csv files from the server\n",
    "! rsync -avze ssh jlasser@medea:/data/honesty/corpora/Twitter/US_politician_timelines_ontology_update/*.xz ../../data/tweets/timeline_batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a4e9a3b-3c7c-49cd-b9d6-a6020ebb4a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../data/tweets/timeline_batches\"\n",
    "fnames = listdir(src)\n",
    "fnames = [f for f in fnames if f.endswith(\".xz\")]\n",
    "timelines = pd.concat([pd.read_csv(Path(src, fname), compression=\"xz\", \n",
    "                                   dtype={\"id\":str, \"author_id\":str}, low_memory=False) \\\n",
    "                       for fname in fnames])\n",
    "timelines = timelines.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af4a36ed-b1c2-4cde-98f1-374f0cf11ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "timelines = timelines[timelines[\"author_id\"].isin(users[\"author_id\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8214545e-a38c-4d38-b5b5-2ec9bef8cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = \"../../data/tweets\"\n",
    "fname = \"combined_US_politician_twitter_timelines_2010-11-06_to_2023-02-11_raw.csv.gzip\"\n",
    "timelines.to_csv(Path(dst, fname), compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efb19a9-05e1-49c8-8bf4-49b6c490f7b5",
   "metadata": {},
   "source": [
    "# Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b29775-41f8-4357-8cac-2f732fe9be2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1312276/1580909548.py:3: DtypeWarning: Columns (19,20,21,23,31,33,34,35,42,43,44,45,46,54,59,60,61,62,63,64,65,66,67,68,69) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  timelines = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "src = \"../../data/tweets\"\n",
    "fname = \"combined_US_politician_twitter_timelines_2010-11-06_to_2023-02-11_raw.csv.gzip\"\n",
    "timelines = pd.read_csv(\n",
    "    Path(src, fname), \n",
    "    compression=\"gzip\", \n",
    "    dtype={\"id\":str, \"author_id\":str})\n",
    "timelines[\"created_at\"] = pd.to_datetime(timelines[\"created_at\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd1eb00b-ad8a-45f7-9bbd-6834b2508894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped 74283 tweets outside the desired time window\n"
     ]
    }
   ],
   "source": [
    "# make sure all tweets are within the desired time window\n",
    "start = pd.to_datetime(\"2010-11-06\", utc=\"UTC\")\n",
    "end = pd.to_datetime(\"2022-12-31\", utc=\"UTC\")\n",
    "N = len(timelines)\n",
    "timelines = timelines[timelines[\"created_at\"] >= start]\n",
    "timelines = timelines[timelines[\"created_at\"] <= end]\n",
    "print(f\"dropped {N - len(timelines)} tweets outside the desired time window\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50c43e27-f9bf-4239-9c23-eb22f840b353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5926497"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(timelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f70c095f-8c72-483d-80fd-404751a4c899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped 47337 duplicates\n"
     ]
    }
   ],
   "source": [
    "N = len(timelines)\n",
    "timelines = timelines.drop_duplicates(subset=\"id\")\n",
    "print(f\"dropped {N - len(timelines)} duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9517f80-d722-4408-a851-a524a89098b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5879160"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(timelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "942a5480-49bc-4414-8a42-0c0d0b11bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tweet type\n",
    "timelines[\"retweeted\"] = False\n",
    "timelines[\"quoted\"] = False\n",
    "timelines[\"reply\"] = False\n",
    "timelines.loc[timelines[\"referenced_tweets.retweeted.id\"].dropna().index, \"retweeted\"] = True\n",
    "timelines.loc[timelines[\"referenced_tweets.quoted.id\"].dropna().index, \"quoted\"] = True\n",
    "timelines.loc[timelines[\"referenced_tweets.replied_to.id\"].dropna().index, \"reply\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c107cb95-13c8-4fed-9d58-3271a1e91765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped 1351346 retweets\n"
     ]
    }
   ],
   "source": [
    "# drop retweets\n",
    "N = len(timelines)\n",
    "timelines = timelines[timelines[\"retweeted\"] == False]\n",
    "print(f\"dropped {N - len(timelines)} retweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3d695b5-a59a-4b3a-b8dc-ddc3f27bf026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4527814 tweets remaining\n"
     ]
    }
   ],
   "source": [
    "# new 2010-11-06 to 2022-03-16: 5392309 tweets remaining\n",
    "# original 2010-11-06 to 2022-03-16 : 2205517 tweets remaining\n",
    "print(f\"{len(timelines)} tweets remaining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28acea3b-19dc-45d9-99aa-1d1b538b3daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "529389 quote tweets\n",
      "572113 replies\n"
     ]
    }
   ],
   "source": [
    "print(\"{} quote tweets\".format(len(timelines[timelines[\"quoted\"] == True])))\n",
    "print(\"{} replies\".format(len(timelines[timelines[\"reply\"] == True])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2812b9ff-cb5d-4739-a50a-6419e549fe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up column names\n",
    "timelines = timelines.rename(columns={\n",
    "    'public_metrics.like_count':'like_count',\n",
    "    'public_metrics.reply_count':'reply_count',\n",
    "    'public_metrics.retweet_count':'retweet_count',\n",
    "    'public_metrics.quote_count':'quote_count',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea2fa255-98a7-4c27-b160-be31a1d89fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract URL objects\n",
    "urls = []\n",
    "expanded_urls = []\n",
    "for obj in timelines[\"entities.urls\"]:\n",
    "    if obj != obj:\n",
    "        urls.append([])\n",
    "        expanded_urls.append([])\n",
    "    else:\n",
    "        obj = eval(obj)\n",
    "        tmp_urls = []\n",
    "        tmp_expanded_urls = []\n",
    "        for entry in obj:\n",
    "            tmp_urls.append(entry[\"url\"])\n",
    "            tmp_expanded_urls.append(entry[\"expanded_url\"])\n",
    "        urls.append(tmp_urls)\n",
    "        expanded_urls.append(tmp_expanded_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9526294f-e782-404e-aa2f-248e36828e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "timelines[\"urls\"] = urls\n",
    "timelines[\"expanded_urls\"] = expanded_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782751e4-a40d-40b1-9176-9b7fc992bed1",
   "metadata": {},
   "source": [
    "# Export the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b1fcd15-2da9-48ac-bd64-6fc557d0ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"combined_US_politician_twitter_timelines_2010-11-06_to_2022-12-31_clean.csv.gzip\"\n",
    "dst = \"../../data/tweets\"\n",
    "cols = [\"id\", \"author_id\", \"created_at\", \"expanded_urls\", \"retweeted\", \"quoted\",\n",
    "        \"reply\", \"text\", \"retweet_count\", \"reply_count\", \"like_count\",\n",
    "        \"quote_count\"]\n",
    "timelines[cols].to_csv(Path(dst, fname), index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67528c7e-ed67-4764-bff6-40525cf0d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export tweet IDs for hydration\n",
    "fname = \"tweet_IDs.txt.xz\"\n",
    "dst = \"../../data/tweets/\"\n",
    "np.savetxt(Path(dst, fname), timelines[\"id\"].values, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c74f8b0-510a-455e-bcad-dfd4eb799416",
   "metadata": {},
   "source": [
    "# Export URLs to unravel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c305c97-f7e1-4639-98d2-2d8f686d43c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain(url):\n",
    "    if url != url:\n",
    "        return np.nan\n",
    "    # trailing \"/\" and spaces\n",
    "    url = url.strip('/').strip()\n",
    "    # transform all domains to lowercase\n",
    "    url = url.lower()\n",
    "    # remove any white spaces\n",
    "    url = url.replace(' ', '')\n",
    "    # if present: remove the protocol\n",
    "    if url.startswith((\"http\", \"https\")):\n",
    "        try:\n",
    "            url = url.split('//')[1]\n",
    "        except IndexError:\n",
    "            print(f\"found malformed URL {url}\")\n",
    "            return np.nan\n",
    "    # remove \"www.\" \n",
    "    url = url.replace('www.', '')\n",
    "    url = url.split(\"/\")[0]\n",
    "    return url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c34842c9-be33-4fff-beac-25d08965c571",
   "metadata": {},
   "outputs": [],
   "source": [
    "URLs = []\n",
    "for url_list in timelines[\"expanded_urls\"]:\n",
    "    URLs.extend(url_list)\n",
    "URLs = pd.DataFrame({\"url\":list(set(URLs))})\n",
    "URLs[\"domain\"] = URLs[\"url\"].apply(extract_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c01a9de6-0a89-4d0e-8522-7c7e5f48f3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial list of shorteners from this repo:\n",
    "# https://github.com/boutetnico/url-shorteners\n",
    "src = \"../../data/utilities\"\n",
    "url_shorteners = list(np.loadtxt(Path(src, \"url_shorteners.txt\"), dtype=str))\n",
    "\n",
    "# add URL shorteners based on manual inspections of all URLs that appeared >100\n",
    "# times in the dataset\n",
    "url_shorteners.extend([\n",
    "    \"fb.me\", \"buff.ly\", \"nyti.ms\", \"wapo.st\", \"youtu.be\", \"1.usa.gov\", \"fxn.ws\",\n",
    "    \"on.fb.me\", \"politi.co\", \"trib.al\", \"washex.am\", \"hill.cm\", \"cnb.cx\",\n",
    "    \"hubs.ly\", \"cs.pn\",\"n.pr\", \"conta.cc\", \"mi.tt\", \"usat.ly\", \"abcn.ws\",\n",
    "    \"reut.rs\", \"cbsn.ws\", \"huff.to\", \"instagr.am\", \"bloom.bg\", \"fw.to\", \n",
    "    \"ift.tt\", \"strib.mn\", \"lat.ms\", \"afs.mn\", \"dpo.st\", \"mailchi.mp\",\n",
    "    \"dailysign.al\", \"tmblr.co\", \"rub.io\", \"yhoo.it\", \"omny.fm\", \"chrl.ie\",\n",
    "    \"tulsi.to\", \"apne.ws\", \"hrc.io\", \"ed.gr\", \"ti.me\", \"herit.ag\", \"indy.st\",\n",
    "    \"ofa.bo\", \"trib.in\", \"azc.cc\", \"bsun.md\", \"wjcf.co\", \"bityl.co\", \"go.shr.lc\",\n",
    "    \"t1p.de\", \"m.bild.de\", \"sz.de\", \"m.faz.net\", \"zpr.io\", \"m.tagesspiegel.de\",\n",
    "    \"to.welt.de\", \"gleft.de\", \"nol.is\", \"m.spiegel.de\", \"m.youtube.com\", \n",
    "    \"m.facebook.com\", \"m.focus.de\", \"loom.ly\", \"t.me\", \"4sq.com\", \"diplo.de\",\n",
    "    \"p.dw.com\", \"owl.li\", \"tmi.me\", \"m.haz.de\", \"ly.zdf.de\", \"chng.it\", \"img.ly\",\n",
    "    \"m.augsburger-allgemeine.de\", \"x.swr.de\", \"m.fr.de\", \"ebx.sh\", \"m.fr.de\",\n",
    "    \"fcld.ly\", \"spoti.fi\", \"shar.es\", \"s.rlp.de\", \"m.welt.de\", \"bbc.in\", \n",
    "    \"on.ft.com\", \"fb.watch\", \"mol.im\", \"crowd.in \", \"zcu.io\", \"gu.com\",\n",
    "    \"lnkd.in\", \"shorturl.at\", \"m.huffingtonpost.co.uk\", \"fal.cn\", \"lght.ly\", \n",
    "    \"econ.st\", \"huffp.st\", \"l-bc.co\", \"wbs.wales\", \"aca.st \", \"ind.pn\", \"cutt.ly\",\n",
    "    \"dailym.ai\", \"ow.ly\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08d942a0-0e80-4e33-9c4b-c5b306d0fa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565035 shortened and 3269841 unshortened URLs\n"
     ]
    }
   ],
   "source": [
    "# original: 139972 shortened and 1160971 unshortened URLs\n",
    "shortened_urls = URLs[URLs[\"domain\"].isin(url_shorteners)]\n",
    "unshortened_urls = URLs[~URLs[\"domain\"].isin(url_shorteners)]\n",
    "print(f\"{len(shortened_urls)} shortened and {len(unshortened_urls)} unshortened URLs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6da6676d-6685-4ae1-92a8-ba61ddb1df76",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"US_politician_URLs.csv.gzip\"\n",
    "dst = \"../../data/urls\"\n",
    "shortened_urls[\"url\"].to_csv(Path(dst, fname),\n",
    "                    compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e0c47705-18b4-4aa0-a2ca-1c3d10304553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending incremental file list\n",
      "US_politician_URLs.csv.gzip\n",
      "\n",
      "sent 4,305,719 bytes  received 35 bytes  1,722,301.60 bytes/sec\n",
      "total size is 4,303,149  speedup is 1.00\n"
     ]
    }
   ],
   "source": [
    "# copy the URLs that need unravelling to the server\n",
    "! rsync -avze ssh ../../data/urls/US_politician_URLs.csv.gzip jlasser@medea:/home/jlasser/Honesty-project/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8800c86a-602f-4793-8ff5-4203078a5cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on the server: follow all the URLs. Note: this takes about a day per 100k URLs\n",
    "! python unravel_urls.py ../../data/urls/US_politician_URLs.csv.gzip -dst ../../data/urls/US_politician_unraveled_urls/ -v 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8629d715-0ebf-4261-a4c7-0fb05070c07c",
   "metadata": {},
   "source": [
    "# Load unraveled URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a88bd8b-5b13-49e1-8728-72831a9c31da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load unraveled URLs from server (they are stored in batches of 1000 URLs\n",
    "! rsync -avze ssh jlasser@medea:/home/jlasser/Honesty-project/data/US_politician_unraveled_urls/ ../../data/urls/US_politician_unraveled_urls/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abca80f5-9838-48ad-956d-96be68717ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5651\n",
      "1000/5651\n",
      "2000/5651\n",
      "3000/5651\n",
      "4000/5651\n",
      "5000/5651\n"
     ]
    }
   ],
   "source": [
    "src = '../../data/urls/US_politician_unraveled_urls'\n",
    "files = listdir(src)\n",
    "unraveled_urls = pd.DataFrame()\n",
    "for i,f in enumerate(files):\n",
    "    if i%1000 == 0:\n",
    "        print(f\"{i}/{len(files)}\")\n",
    "    tmp = pd.read_csv(Path(src, f), compression=\"gzip\")\n",
    "    unraveled_urls = pd.concat([unraveled_urls, tmp])\n",
    "unraveled_urls = unraveled_urls.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c44db3-764a-4d8c-8a9e-28a093ce4ade",
   "metadata": {},
   "source": [
    "# Add hosts from timeouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6051761-5238-4e68-b50d-3cb5989fad11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45559 timeouts (8.77%)\n"
     ]
    }
   ],
   "source": [
    "timeouts = len(unraveled_urls) - len(unraveled_urls[\"status_code\"].dropna())\n",
    "print(\"{} timeouts ({:1.2f}%)\".format(\\\n",
    "        timeouts,\n",
    "        (timeouts / len(unraveled_urls[\"status_code\"].dropna()) * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "871e0b7e-d69c-4052-aea2-3b2b226e0f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_host(unraveled_url):\n",
    "    if unraveled_url == unraveled_url and unraveled_url.startswith(\"Cannot\"):\n",
    "        host = unraveled_url.split(\" \")[4].split(\":\")[0]\n",
    "        return host\n",
    "    else:\n",
    "        return unraveled_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a624906e-827d-4860-814f-62b43d055475",
   "metadata": {},
   "outputs": [],
   "source": [
    "unraveled_urls[\"unraveled_url\"] = unraveled_urls[\"unraveled_url\"].apply(extract_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b61e2794-c6d9-42c6-8a84-2374e8a62e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = '../../data/urls'\n",
    "unraveled_urls.to_csv(Path(dst, \"US_unraveled_urls.csv.xz\"), index=False,\n",
    "                      compression=\"xz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d00fdc-fa0e-417d-b11d-33919f3cd792",
   "metadata": {},
   "source": [
    "# Timeline updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2ed893-e47b-4a9a-8ace-7a760e1c1d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commands on medea to download the user timelines for each batch using twarc2\n",
    "cd /data/honesty/corpora/Twitter/US_politician_twitter_accounts_ontology\n",
    "twarc2 --bearer-token XXX timelines --no-context-annotations --start-time <new_start_time> --end-time <new_end_time> US_politician_twitter_accounts_batch_0.txt ../US_politician_timelines_update/US_politician_twitter_accounts_batch_0.jsonl\n",
    "twarc2 --bearer-token XXX timelines --no-context-annotations --start-time <new_start_time> --end-time <new_end_time> US_politician_twitter_accounts_batch_1.txt ../US_politician_timelines_update/US_politician_twitter_accounts_batch_1.jsonl\n",
    "twarc2 --bearer-token XXX timelines --no-context-annotations --start-time <new_start_time> --end-time <new_end_time> US_politician_twitter_accounts_batch_2.txt ../US_politician_timelines_update/US_politician_twitter_accounts_batch_2.jsonl\n",
    "twarc2 --bearer-token XXX timelines --no-context-annotations --start-time <new_start_time> --end-time <new_end_time> US_politician_twitter_accounts_batch_3.txt ../US_politician_timelines_update/US_politician_twitter_accounts_batch_3.jsonl\n",
    "twarc2 --bearer-token XXX timelines --no-context-annotations --start-time <new_start_time> --end-time <new_end_time> US_politician_twitter_accounts_batch_4.txt ../US_politician_timelines_update/US_politician_twitter_accounts_batch_4.jsonl\n",
    "twarc2 --bearer-token XXX timelines --no-context-annotations --start-time <new_start_time> --end-time <new_end_time> US_politician_twitter_accounts_batch_5.txt ../US_politician_timelines_update/US_politician_twitter_accounts_batch_5.jsonl\n",
    "twarc2 --bearer-token XXX timelines --no-context-annotations --start-time <new_start_time> --end-time <new_end_time> US_politician_twitter_accounts_batch_6.txt ../US_politician_timelines_update/US_politician_twitter_accounts_batch_6.jsonl\n",
    "twarc2 --bearer-token XXX timelines --no-context-annotations --start-time <new_start_time> --end-time <new_end_time> US_politician_twitter_accounts_batch_7.txt ../US_politician_timelines_update/US_politician_twitter_accounts_batch_7.jsonl\n",
    "twarc2 --bearer-token XXX timelines --no-context-annotations --start-time <new_start_time> --end-time <new_end_time> US_politician_twitter_accounts_batch_8.txt ../US_politician_timelines_update/US_politician_twitter_accounts_batch_8.jsonl\n",
    "twarc2 --bearer-token XXX timelines --no-context-annotations --start-time <new_start_time> --end-time <new_end_time> US_politician_twitter_accounts_batch_9.txt ../US_politician_timelines_update/US_politician_twitter_accounts_batch_9.jsonl\n",
    "twarc2 --bearer-token XXX timelines --no-context-annotations --start-time <new_start_time> --end-time <new_end_time> US_politician_twitter_accounts_batch_10.txt ../US_politician_timelines_update/US_politician_twitter_accounts_batch_10.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a919e-4fb9-4d99-906a-a279564ad142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commands on the server to convert the raw .json files to .csv\n",
    "cd ../US_politician_timelines_update\n",
    "twarc2 csv --input-data-type tweets US_politician_twitter_accounts_batch_0.jsonl US_politician_twitter_accounts_batch_0.csv\n",
    "twarc2 csv --input-data-type tweets US_politician_twitter_accounts_batch_1.jsonl US_politician_twitter_accounts_batch_1.csv\n",
    "twarc2 csv --input-data-type tweets US_politician_twitter_accounts_batch_2.jsonl US_politician_twitter_accounts_batch_2.csv\n",
    "twarc2 csv --input-data-type tweets US_politician_twitter_accounts_batch_3.jsonl US_politician_twitter_accounts_batch_3.csv\n",
    "twarc2 csv --input-data-type tweets US_politician_twitter_accounts_batch_4.jsonl US_politician_twitter_accounts_batch_4.csv\n",
    "twarc2 csv --input-data-type tweets US_politician_twitter_accounts_batch_5.jsonl US_politician_twitter_accounts_batch_5.csv\n",
    "twarc2 csv --input-data-type tweets US_politician_twitter_accounts_batch_6.jsonl US_politician_twitter_accounts_batch_6.csv\n",
    "twarc2 csv --input-data-type tweets US_politician_twitter_accounts_batch_7.jsonl US_politician_twitter_accounts_batch_7.csv\n",
    "twarc2 csv --input-data-type tweets US_politician_twitter_accounts_batch_8.jsonl US_politician_twitter_accounts_batch_8.csv\n",
    "twarc2 csv --input-data-type tweets US_politician_twitter_accounts_batch_9.jsonl US_politician_twitter_accounts_batch_9.csv\n",
    "twarc2 csv --input-data-type tweets US_politician_twitter_accounts_batch_10.jsonl US_politician_twitter_accounts_batch_10.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0cc522-4eeb-4679-809d-20141bf9252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commands on the server to compress the .csv files\n",
    "xz -v US_politician_twitter_accounts_batch_0.csv\n",
    "xz -v US_politician_twitter_accounts_batch_1.csv\n",
    "xz -v US_politician_twitter_accounts_batch_2.csv\n",
    "xz -v US_politician_twitter_accounts_batch_3.csv\n",
    "xz -v US_politician_twitter_accounts_batch_4.csv\n",
    "xz -v US_politician_twitter_accounts_batch_5.csv\n",
    "xz -v US_politician_twitter_accounts_batch_6.csv\n",
    "xz -v US_politician_twitter_accounts_batch_7.csv\n",
    "xz -v US_politician_twitter_accounts_batch_8.csv\n",
    "xz -v US_politician_twitter_accounts_batch_9.csv\n",
    "xz -v US_politician_twitter_accounts_batch_10.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f918190-05a0-4ac6-9058-cd833adc9bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ../../data/tweets/timeline_batches_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5741a7b0-d21f-49d2-b94a-2655eae35e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the .csv files from the server\n",
    "! rsync -avze ssh jlasser@medea:/data/honesty/corpora/Twitter/US_politician_timelines_update/*.xz ../../data/tweets/timeline_batches_update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a187e7-2487-435a-9b6e-fc80fea81c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../data/tweets/timeline_batches_update\"\n",
    "fnames = listdir(src)\n",
    "fnames = [f for f in fnames if f.endswith(\".xz\")]\n",
    "timelines = pd.concat([pd.read_csv(Path(src, fname), compression=\"xz\", \n",
    "                                   dtype={\"id\":str, \"author_id\":str}) \\\n",
    "                       for fname in fnames])\n",
    "timelines = timelines.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48d661cd-d229-4062-b514-0ee9ea6bc29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "timelines = timelines[timelines[\"author_id\"].isin(users[\"author_id\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76acefec-d934-4f93-af4e-f1c720654578",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = \"../../data/tweets\"\n",
    "new_start_time = # insert\n",
    "new_end_time = # insert\n",
    "fname = f\"combined_US_politician_twitter_timelines_{new_start_time}_to_{new_end_time}_raw.csv.gzip\"\n",
    "timelines.to_csv(Path(dst, fname), compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5cb9d3-54bb-439d-85cb-53215e7a8362",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbaa0e7a-b3fa-4da4-b38d-c5031274d4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_151773/145073831.py:3: DtypeWarning: Columns (20,21,23,31,33,34,35,42,44,59,60,61,62,63,64,65,66,67,68,69) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  timelines = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "src = \"../../data/tweets\"\n",
    "fname = f\"combined_US_politician_twitter_timelines_{new_start_time}_to_{new_end_time}_raw.csv.gzip\"\n",
    "timelines = pd.read_csv(\n",
    "    Path(src, fname), \n",
    "    compression=\"gzip\", \n",
    "    dtype={\"id\":str, \"author_id\":str, \"conversation_id\":str})\n",
    "timelines[\"created_at\"] = pd.to_datetime(timelines[\"created_at\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a89024d4-d5e7-44ae-a70f-201bbfbc569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tweet type\n",
    "timelines[\"retweeted\"] = False\n",
    "timelines[\"quoted\"] = False\n",
    "timelines[\"reply\"] = False\n",
    "timelines.loc[timelines[\"referenced_tweets.retweeted.id\"].dropna().index, \"retweeted\"] = True\n",
    "timelines.loc[timelines[\"referenced_tweets.quoted.id\"].dropna().index, \"quoted\"] = True\n",
    "timelines.loc[timelines[\"referenced_tweets.replied_to.id\"].dropna().index, \"reply\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db122f97-2654-4e09-8c76-126947902c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170466 tweets in update\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(timelines)} tweets in update\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da26baa7-3737-4555-b69e-fbdd034b5f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up column names\n",
    "timelines = timelines.rename(columns={\n",
    "    'public_metrics.like_count':'like_count',\n",
    "    'public_metrics.reply_count':'reply_count',\n",
    "    'public_metrics.retweet_count':'retweet_count',\n",
    "    'public_metrics.quote_count':'quote_count',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3270bbd4-78fd-4d7a-b2ee-b78068d85ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract URL objects\n",
    "urls = []\n",
    "expanded_urls = []\n",
    "for obj in timelines[\"entities.urls\"]:\n",
    "    if obj != obj:\n",
    "        urls.append([])\n",
    "        expanded_urls.append([])\n",
    "    else:\n",
    "        obj = eval(obj)\n",
    "        tmp_urls = []\n",
    "        tmp_expanded_urls = []\n",
    "        for entry in obj:\n",
    "            tmp_urls.append(entry[\"url\"])\n",
    "            tmp_expanded_urls.append(entry[\"expanded_url\"])\n",
    "        urls.append(tmp_urls)\n",
    "        expanded_urls.append(tmp_expanded_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f2a4942-becc-495b-b496-ce52d0b70c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timelines[\"urls\"] = urls\n",
    "timelines[\"expanded_urls\"] = expanded_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde696e1-8ad4-49fd-b27c-0514974f0c75",
   "metadata": {},
   "source": [
    "## Export the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c335afbc-9f9b-4b79-8f08-639da3a3f98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = f\"combined_US_politician_twitter_timelines_{new_start_time}_to_{new_end_time}_clean.csv.gzip\"\n",
    "dst = \"../../data/tweets\"\n",
    "cols = [\"id\", \"author_id\", \"created_at\", \"expanded_urls\", \"retweeted\", \"quoted\",\n",
    "        \"reply\", \"text\", \"retweet_count\", \"reply_count\", \"like_count\",\n",
    "        \"quote_count\"]\n",
    "timelines[cols].to_csv(Path(dst, fname), index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92b0d596-479a-44d3-8df5-a63da4a43393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export tweet IDs for hydration\n",
    "fname = \"tweet_IDs_update.txt.xz\"\n",
    "dst = \"../../data/tweets/\"\n",
    "np.savetxt(Path(dst, fname), timelines[\"id\"].values, fmt=\"%s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
