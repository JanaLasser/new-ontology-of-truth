{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a6b1c8e-49b2-44a5-946a-e1da1a02cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b109ceb-d626-465f-ac9e-b14ea980ccbe",
   "metadata": {},
   "source": [
    "# Hydrate tweets from tweet IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842193cd-3657-4028-93d6-ad8118e11b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# US\n",
    "# 2,717,708 tweet IDs\n",
    "# takes about 8 hours as the rate limit for hydrating is 360,000 tweets / hour\n",
    "# nees about 8 GB of space on hard drive\n",
    "! twarc2 hydrate ../../data/tweets/combined_US_politician_twitter_timelines_2010-11-06_to_2022-03-16_clean_tweetIDs.txt ../../data/tweets/combined_US_politician_twitter_timelines_2010-11-06_to_2022-03-16.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53cb0ea-b7bb-4c60-a2ca-b8b346193d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert JSON to csv\n",
    "# needs about 6 GB uncompressed\n",
    "! twarc2 csv --input-data-type tweets ../../data/tweets/combined_US_politician_twitter_timelines_2010-11-06_to_2022-03-16.jsonl ../../data/tweets/combined_US_politician_twitter_timelines_2010-11-06_to_2022-03-16_hydrated.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f75c6560-41a4-4fa9-bbf6-feca6bcc81eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up JSON \n",
    "! rm ../../data/tweets/combined_US_politician_twitter_timelines_2010-11-06_to_2022-03-16.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9602501a-463b-44a1-b3b7-83dad37da684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compress csv\n",
    "! xz -v ../../data/tweets/combined_US_politician_twitter_timelines_2010-11-06_to_2022-03-16_hydrated.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48150e41-0428-4b87-aad9-a51391d281a4",
   "metadata": {},
   "source": [
    "# Clean hydrated tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f53939a-970c-415d-8e4e-6061c0ab53e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls(tweets):\n",
    "    '''Extracts URLs from the JSON objects the twitter API returns'''\n",
    "    urls = []\n",
    "    expanded_urls = []\n",
    "    for obj in tweets[\"entities.urls\"]:\n",
    "        if obj != obj:\n",
    "            urls.append([])\n",
    "            expanded_urls.append([])\n",
    "        else:\n",
    "            obj = eval(obj)\n",
    "            tmp_urls = []\n",
    "            tmp_expanded_urls = []\n",
    "            for entry in obj:\n",
    "                tmp_urls.append(entry[\"url\"])\n",
    "                tmp_expanded_urls.append(entry[\"expanded_url\"])\n",
    "            urls.append(tmp_urls)\n",
    "            expanded_urls.append(tmp_expanded_urls)\n",
    "            \n",
    "    return urls, expanded_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6c81e9-c434-42dd-8a4d-66c99c901fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../data/tweets\"\n",
    "fname = \"combined_US_politician_twitter_timelines_2010-11-06_to_2022-03-16_hydrated.csv.xz\"\n",
    "\n",
    "tweets = pd.read_csv(join(src, fname.format(country)),\n",
    "                     dtype={\"id\":str, \"author_id\":str},\n",
    "                     compression=\"xz\")\n",
    "\n",
    "print(f\"{country}: {len(tweets)} tweets\")\n",
    "\n",
    "tweets[\"retweeted\"] = False\n",
    "tweets[\"quoted\"] = False\n",
    "tweets[\"reply\"] = False\n",
    "tweets.loc[tweets[\"referenced_tweets.retweeted.id\"].dropna().index, \"retweeted\"] = True\n",
    "tweets.loc[tweets[\"referenced_tweets.quoted.id\"].dropna().index, \"quoted\"] = True\n",
    "tweets.loc[tweets[\"referenced_tweets.replied_to.id\"].dropna().index, \"reply\"] = True\n",
    "\n",
    "# clean up column names\n",
    "tweets = tweets.rename(columns={\n",
    "    'public_metrics.like_count':'like_count',\n",
    "    'public_metrics.reply_count':'reply_count',\n",
    "    'public_metrics.retweet_count':'retweet_count',\n",
    "    'public_metrics.quote_count':'quote_count',\n",
    "})\n",
    "\n",
    "# extract URLs\n",
    "urls, expanded_urls = extract_urls(tweets)\n",
    "tweets[\"urls\"] = urls\n",
    "tweets[\"expanded_urls\"] = expanded_urls\n",
    "\n",
    "# dump data\n",
    "fname = \"combined_US_politician_twitter_timelines_2010-11-06_to_2022-03-16_clean.csv.gzip\"\n",
    "cols = [\"id\", \"author_id\", \"created_at\", \"expanded_urls\", \"retweeted\",\n",
    "        \"quoted\", \"reply\", \"text\", \"retweet_count\", \"reply_count\", \n",
    "        \"like_count\", \"quote_count\"]\n",
    "tweets[cols].to_csv(join(src, fname), index=False, compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
